{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "* numpy - package for scientific computing with Python\n",
    "* matplotlib.pyplot - plotting framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras models\n",
    "* Sequential - basic keras model composed of a linear stack of layers.\n",
    "* model_from_json - Loads a saved model from a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/sviras/anaconda3/envs/tensorflow-env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/sviras/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/pydot.py:17: UserWarning: Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "  \"Couldn't import dot_parser, \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras Layers\n",
    "* Conv2D- 2D convolution layer\n",
    "* MaxPooling2D - Max pooling operation for spatial data.\n",
    "* Flatten - Flattens the input. Does not affect the batch size.\n",
    "* Dense - regular densely-connected NN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessing packages\n",
    "* image - image preprocessing package\n",
    "* ImageDataGenerator - Generate batches of tensor image data with real-time data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the classifier model\n",
    "* Build a CNN.CNN has mostly four fucntions:\n",
    "    * Convolution: Add the first layer which is a convolutional layer. Set the number of filters as 32, the shape of each filter as 3x3 and the input shape and the type of image as 50,50,3 i.e. the input is of a 50x50 RGB image and the activation function as relu.\n",
    "    * Pooling: Add a pooling layer to reduce the total number of nodes for the upcoming layers. It takes a 2x2 matrix thus giving minimum pixel loss and a precise region where the features are located.\n",
    "    * Flatten : Flattens the pooled images.\n",
    "    * Dense : add a fully connected layer to feed the images to the output layer. Set the number of nodes as 256, as its a common practice to use a power of 2 and a rectifier function asthe activation function, relu.\n",
    "\n",
    "* Define the output layer. Set number of units to 1 as this is a binary classifier and sigmoid as the activation function\n",
    "* Compile the model. Set adam as the optimizer and binary_crossentropy as the loss fucntion, as this is a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape = (50, 50, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 256, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the CNN to the images\n",
    "* Improve the dataset using the ImageDataGenerator method which generate batches of tensor image data with real-time data augmentation. \n",
    "    * rescale: rescaling factor. If None or 0, no rescaling is applied, otherwise the data is multiplied by the value provided.\n",
    "    * shear_range: Shear Intensity\n",
    "    * zoom_range: Range for random zoom.\n",
    "    * horizontal_flip: Randomly flip inputs horizontally if true.\n",
    "* Define the training and test datasets using the flow_from_directory which takes the path to a directory, and generates batches of augmented/normalized data.\n",
    "    * directory: path to the target directory. It should contain one subdirectory per class.\n",
    "    * target_size: The dimensions to which all images found will be resized.\n",
    "    * class_mode: one of \"categorical\", \"binary\", \"sparse\", \"input\" or None. Determines the type of label arrays that are returned\n",
    "    * batch_size: size of the batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "shear_range = 0.2,\n",
    "zoom_range = 0.2,\n",
    "horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8005 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "target_size = (50, 50),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2023 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "target_size = (50, 50),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "* Print a preprocessed image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnWuMZVl13//rPO65z3p3d/VzpgeGx2BhiEdkZJIIYZOMMTJ8cCQTJyIS0nxJJKw4MpBIUSzlg/3F8CVyhIzl+WCBjbEEQpYiMgE5wTHvwTAzZt7TXU13V1XX69Z9nOfOh7qDa621u6q6p+d2NWf9pFb1vvfsc/bd5+x77lpnrf8i5xwMw6gXwd0egGEY08cWvmHUEFv4hlFDbOEbRg2xhW8YNcQWvmHUEFv4hlFDbOEbRg15TQufiB4loh8T0fNE9Ik7NSjDMF5f6HYj94goBPAsgPcBWAHwbQAfds49fbM+URi6OI5v6TgBEWv7xuvgxDaeHYnXoihk7TD0fQfyTlVZsnZRVKpH3OD7hRh/0mioPmXF91uWfL9hIPYJIAj4fuVx9raRn4l/ntForPo04kjuhbXSbKT6yLkj8PHmeaH6AHy85Bl/WfJ+8tz7+sjXqorPZdzQ1x85vZ/9FKUef7OZ8OOU8hrUF2Epr59SXz8k5lKee99n3s84TZHn+cEbAZBn+VZ4F4DnnXMvTgb0eQAfBHDThR/HMR6479yBO5Ujbkb8RJV5rvrkxCewKPSkVznfZnFhjrVn5tqqT4SMtfvbA9beuLGr+iyf5/uVi+++++5TfXaGO6y9u8MXV6czo/q0mvwLJAz1l0Or02Jt5/gcPPXUU6rP+eVl/kLQZM0XVnSf+S6fu4A6rH3tyobqE4aJaOvxb23xflnOv6iaTT42AAgiPt/D4ZC1z53T11/k+DKQXxabm3r8b3nrG1h7sJvysaZ8kQPA1vY2a9/o99U2zU6PtWdmZllbf5lznvzhjw58/6f7OdJWfs4CuLyvvTJ5zTCMY85rueP7fk6oWy0RPQbgMQCIo9dyOMMw7hSv5Y6/AuD8vvY5AD+RGznnPuOce9g597Dv55xhGNPntSz8bwN4kIguElEDwG8A+PKdGZZhGK8nt/3b2zlXENG/B/A/AYQA/tg5pz0/hmEcO16T0e2c+ysAf3WHxmIYxpSwyD3DqCG28A2jhtjCN4waYgvfMGqILXzDqCFTDqVzN8mg2YdIQghIfDd5YpVTmUThOUQggocKkRiTFzoRo3J8GyeSOcLQl3DExxfHPKY+zXRiTJ7p/IP9FJ78BCeSTeQ0AQAqkTiCw5Nc8px/5uGYx5PnmY5BH4x4nHoS8xj6yhPkKYdbea6LSCR0lRDJTJ4+8kgkrxfPZ5ZJODLBJk70eZaJVZXIg5B5EXvH5vsNZaIVPElppdyv3i1//2hJd3bHN4waYgvfMGqILXzDqCFTtfEJQFh5bJ/92ygbn78fNbWQRV/kr5faDEUs8vp3dnlufUWecQmhjUB8TybdrupSljzPvDezwNrX11ZVn1zY8M0mz29Px9ov0GnwXPtICWgApRCHkH6NONb57KvXec741dU11m7P8lx7ALi6xv0AzabIb/cIXRQp/0xh5NETmOG56ZTzc9/vaz0EeZ6bHT4Wj1QDhmO+n0hkkTY6/JwCwNoGz9GXvg+vYEbED97ptdQmccDHnw/5tX2YDe8OWV+vYnd8w6ghtvANo4bYwjeMGmIL3zBqyFSde2EQYKGrnUMHUQhnxmCsVV6l4myW6oCXNOWii2HMnUkd0s6lUZ6JV7jjZH5Bi2Bu9blwJoT4I5x2TkaBcNSF3OnW6WknHInxpx6nTiimoayk4OiS6vP3K8+w9vkLp1l7q8+DdQBgToxvPOLz5gtm2R5wh2CjoR1oXRF05Sp+nuPQM5fEX4uEam1Q6ntdAO5Qq0Qs19BzPRVCvLUSncLEozYlFHMbgR6/PI3jjM+lVOrV/c25ZxjGTbCFbxg1xBa+YdSQ6Qfw+JIX9iGVeHsiSGb7qi5CIPt0PTZxXogKK+J9n+mUV9y2c+DtVm9R9Vlfv873u8GP22lpu7orAnbGQxFok+hgkEHOfR2+CkVzPT538lu+3db+lu0dHsDz5p+7yNrPvnAZknMnuR8AofQ/aBs5FOONPBVucmlbi3OUBLpPN5L+Bj5PYaTnsgHepxAJWypRDEAu0oGKSgSReSKFRin3jzRDPZY4FolUIkGoEWhfCNv+kIIbr2J3fMOoIbbwDaOG2MI3jBpiC98washUnXtlBWyPDs4uooB7cKKKZ9Hlle7fTrhzxnnKGruAZ4Mlbe5QKwvt3Usi7vySATCbm74+3KGWZdyh027qAKTBrnAMCWfYqVMPqj7X1rkTLuhqR1EnEY6gSmQo7ohgIwAXLp5n7d2BcEjFOjAl6fC5dDvcATv2OE7bCZ9bX9ZlOuTnrCqEc7WjnZO5zAQUQTKDsXY0RpGs3MvfLwoZyAWkOZ+XouT7jUPthAuEYlPU1NtUjl+7wwHPHPQq++xDKgPdDLvjG0YNsYVvGDXEFr5h1JCp2vhRFGFx4eSB20g7emNni7UdtC3oAm73NxraDm2U3L7qtrl9VRYexRTi22xscpt4uKnt9SgRKjHDG6y9fE4n9pQi7+W0SJ5JM+7nAICNLb7fNNf24qmFedYORHBH4fGXxMLuvHSJVz73BQpJWzsWdnbuiY7a3ObndV74XACgu8jVi65ceoW1m4VH/Tbg/p6Vn/CAquXTItgIQLfBl0Em7OjSo4ZLmVBnElPpU0aOE/4Zg0Bfp6lIQgucVEY++F7tuYq92B3fMGqILXzDqCG28A2jhkz5OX6J3VQn2exHWZ3iubErtV1UQiSFeOyrYsyfxRJ4W1a8AYCwwcdagNuludPfm0XGRTVm5rid/fIlneQiEzpykcxxZvmM6hNJP4ZH9yGr+H6cOE7Tox5749lN1i6qw6sHjYVdurXGlXmTlk9Nll966VD7S0gYzt0O38+sZ78t4SuYfQuPgfAJVWQ5jxe4snaVtRtdPU+yqlIiYiYyT3WkUGQZkdPbNEWyUtg6OClHElxfP9p2t7RXwzB+JrCFbxg15NCFT0R/TESrRPSjfa8tENFXiei5yd/5g/ZhGMbx4ih3/D8B8Kh47RMAnnDOPQjgiUnbMIx7hEOde865vyai+8XLHwTwnsn/HwfwdQAfP2xfeZXjav8nh2x1cJLFONVBJ6kIpmg29McKwJ0+WwOhihp6nD6lKF9EfB8FaUdX4Hifvgry0U7EoCFLU4uED48C7cIJHtzSank+c8z3K4NKnv7xjyAZC9Ub6dxzUgYHwKYoJ7UoAm+efvpp1afVFKW0nU6sKhx3uvWEc6/Z0HOZD3mQlVRnaoR6nmSyTLfHA5I2xzqZqRvx8l4N6ZTzieyKBLQk0WNpRPwa297kyVjjVKsc76c6RIX3p2M50laaU865qwAw+XtwOJ5hGMeK1/1xHhE9BuAxAAilxrxhGHeF212J14noNABM/uoSsBOcc59xzj3snHs4DG3hG8Zx4Hbv+F8G8BEAvzf5+6WjdCKEiMLZA7dxInFkJGzxpqe0sxPBOKVH68ORE9vw/ebaxFTKu40Gt3cbgadiT8ZtvSQRQRuyvA0AabZlQkikkhkgALJdUYmm1KeyOcMTgsai2ks30Ykx4SK3o5cWuZLw8gl9/gZbPImo0+HHPb2sjzMc8D7DoQ7skklEYzFPo9xTPUj4DgpxEgtPAM9glwctuYrfoGY72pINwQeT5fzzRB4130Rcu+3IowYtLsSeqDw1K0qHS67e2Djw/Vc5yuO8zwH4fwDeTEQrRPRR7C349xHRcwDeN2kbhnGPcBSv/odv8tYv3eGxGIYxJczoNowaMt1KOgRE0cFimzKJIm4IW7zk4oMAACmwSPoYpXr+fPA4AP0MOBXGOJG244aiUqx8vl7m+riBSFjZ2uLPjS+/sqL63H/+LH/Bs9/BDt/vQFTo6Xa0vXiyIyrSBkJ8ZFvHYexs8XN0bUUkVnnuL2cu8MSj3rzHd5Pzfi9f4sk//UqLYFbiHGWivTSng0yLMR+vKKSD2JOYNNMWgiu7PIHLd21EIh4j8AiXRjLhLOWDKX0OrH24wy/rvWMfbTPDMH6WsIVvGDXEFr5h1BBb+IZRQ6bq3AMAVx2iAypVbYSjriq1Q0fKz6Sp3iYQSqnS+SIdeT6k41Gq1gKAFKEtS5kMpB1FyvklEkmGY/15rq5ypZVOU6vRVI5/pvGYJ72cOKXLfF+9zBWCFub52FyuVYKHo7Fo83MWxnqeNrZ4wE6S+Mpki2AbcY5crudlLCrc9ESpcFkBBwAaCd+vVNPxxAlhfZ07PWWSTkWepC9RrSnPtaNaOvdykVglVaglPoUhH3bHN4waYgvfMGqILXzDqCFTtfGdIxTFrX3XhEKIgzyJMRCVT4rCs434qGXJ+zQ8og7S1itFwofzREs0RdXXLBMBGF4TTPgfhJ13Q4gxAMCw4Hb1Gy7qCjHDnCdsJKLKz+aGFpi4cp0nWobxHGu7sbbxdwtROTbk8+JkqSAA6Rq3z+fn9X43trkfYFdUz+20ddBPW5zHSvgBqkKfgGGfz+/JpWXWbpK+NjIhxOFEldrCaf+DtPETTyIPiF8vrbbwPdHBvqgwPFotHbvjG0YNsYVvGDXEFr5h1JAp2/gVymp8yEa8WRRSMFJ3yfND9gmARGVS+dzeZ6/LeAC9ibanpKClLLTqixeQfoBM7UN/P2djPpjtTf1M+L7z/Dn90iy3S194Xlf1GezIZ/Lcz3H9mq7UUggRyaIQ8wZtV0tBjDLQgqJFxucqE/EBcz0tCnJj/RrfZpY/x/eEUWDhFPeP5EIAJE2Hqk/S4McOhb1eQV+onS4/eODxA+SiMnLc4Oc+jjwfYP8+PZV9vdsdaSvDMH6msIVvGDXEFr5h1BBb+IZRQ6aepHOY68EJ7550qIWeSiiyOo3PURd5++0bl0cxpRJOnqME8FSi9HEssnZyT2KJVBCSY8k9QSflLj/O1sZAbXPhDA++aQvFl54nMWamx4NXXnyeK8u0elq1Jx2KEuQieSkrPQq6wuHX39XzEovgrTDoiPf1WM6c4slKfRGcs9vX8zS/IJSKhAJwp8sdhACQNPi5H4jz4cuVEflaqDwqUGNxfTRafA4iT/Wd/ZA59wzDuBm28A2jhtjCN4waMnUbvywPtkGk3SwTbqpKl7yRghi+hBtpN8vj+AJrZCJPlnH7q5ByrADimE9pFcjj6u9aGXQhP4/zVJJttniCyv3336e2mZ/lNvG1lSusffnlV1Sf3RGf76jNVWkXz+iqOM8+zfcTiGpBpdPBLJGoeJO0dCLPjRvXWbspqtoWHkXZxQU+3mtXeZBS6fGXBMFFPhZRYWhtVQctzc3zAB4ZbDQeewYnLv24pa+FRiKCiVKR9DU+WGijPGISnN3xDaOG2MI3jBpiC98wasiUbXxSIhO+bfYjxSrTTCdMxJFO8JBIu9knlKn6iOSYnniGXXhK7Aaq+g5PevHFC4zFNrJicKet7eos40k5T/7g23oseCNrv+kCr77zzLPaxi8q6V/gl4gvSWde2LtjIQ46HOtzBsdjCK5d1WIjhRJL4XPrq0okn5WfPXOBtQk6duHqGhcfOXGSV8dNPEKmruRjqTLhGwll5SZdfXnU1/NSCPEUEteC5/Jh5IVPjFZjd3zDqCG28A2jhtjCN4waYgvfMGrIVJ17VVVgMNy8pT7SGdZuddQ2lQhwKTyqrpFQJw0DqV6qAyPGY1EW24lkINLT12pyR1wiKqz4xtZt8+/fSDjU5md0aedM7Kc366nQIxx1Mj8lTXUfUko4I37ckScBKuHBOKVIwGkk+pw5xz1dTY9irlRfqkTwU5noPsOcn9dWi6v3Xlt5WfVZ6vLxkQjuWt/SSUblUFbs4Y5fXzl4eYmFHgdzKALApGiuzzm8H1PgMQzjptjCN4wacujCJ6LzRPQ1InqGiJ4ioo9NXl8goq8S0XOTv/r3qGEYx5Kj2PgFgN92zn2PiHoAvktEXwXwbwE84Zz7PSL6BIBPAPj4ax2QTJ6R7SDQQw5kdRGP7RRF3B6MhNyqTMgBgCDhdlwlIjDKVB+nFP4GWQjloTe+SfU5deoUazsVtKTFI0apVNXViTBlxedqZ1va/HouXcLnIc34HLRa2q6WyOCo3BPoVAkbv9XSQTKNBvc3pCkfS1noIJkgEoE0ER/LcKwDhXozQrxjh9v0wy09/+eEyIm08UtPApesfLs98lV8EqIswlXgE3+5HQ694zvnrjrnvjf5fx/AMwDOAvgggMcnmz0O4EN3ZESGYbzu3JKNT0T3A3gngG8COOWcuwrsfTkAOHnznoZhHCeO/DiPiLoAvgjgt5xzO4c9VtjX7zEAjwFHi483DOP150grkYhi7C36P3XO/eXk5etEdHry/mkAq76+zrnPOOceds49fNRnjIZhvL4cesenvVv7ZwE845z7g31vfRnARwD83uTvl45ywMOcE/KXhP5lob88pKpN4Pk1Ip15pMoN63G1Wwt8C5EN1vZkzTUS7nBanufBIctLni8/d4k1CzFH/V3tXNre4ZldUaSzHtsd7nBKZfBQU2dyBSLTMZLZbJ5fbdJpJZWJolhfZlLB2K9mxI8lr4XKI2Wb53xeqkxmOmrF3PGI76fV4M6+i2e1mm9XlMPq93nJ8dxT6y0IxXVaeVSaVZ/Dsllvj6P81H83gH8D4IdE9OTktf+EvQX/50T0UQCXAPzL12WEhmHccQ5d+M65/4uby+H/0p0djmEY08C8bYZRQ6ZcJvtwG18H7EhFGF9/qa6j7aKqEsE3FbfBfE8pTp/iVWUaIjhnsa2TXDoxtw8zx4NBRjkv4wwAUcAtuwLcDs18tqAYbtDQKkRRzJOKNnZ48ErqUZydm+F+i2yH265p7lHM9SQr7SfPdB9fspJEnsfDlJIB7SsIxCW+fIor8gDA5hb3dSyf5EGo6Vgn6fSFj0VcXnClRx1IbOQ8irgVycAmfg5lZSZJGB7tXm53fMOoIbbwDaOG2MI3jBoy9Uo68Ng++1HP8cX7spouoIrNovIlb8hnqELtd/mUTi588AK38fPdG6wdqlgAYHuDV3+JQj7eyPNd25jldnUlqrIkkRayyMXz3W5rTm2TjrgdPRwIxd9Az5OsdNtrcptyNNbP2/fiu/4B+ew5SvRndiMhqiGNZHiqHYlrIx1xkRAACETcQaPDfS7kqZo8HPHzutMXoi3q6TqQDrmNL6v6OE+wWiiqEwelx18i5q4nBEoaycGK0keNjrU7vmHUEFv4hlFDbOEbRg2xhW8YNWSqzj0CIQgPdk5Ih04uHHVxwzdk7kiJPJvIgJGeUHx50wO8vBQAnF3igTQvrIuSUy2dvDEUDpuOKNldeqZ8OOQBJLsjHjCS5r7EGD4vw11dGhwiMCgTwTe+hJV2k5fDkr7U+89q2YUr13nZp4FQ7YkTPbaucLq1PSq7u30+D4MBb5eekunNDnfS7oo8pKLSwThRizsJpRpxDD3+hnBYlim/BnPnSa4R6slhpB2AQSDUl8Z8bHl2cOBT5VGS8mF3fMOoIbbwDaOG2MI3jBoy3QAeIlB08HeNVBWt5Ave7qLiiifgIhOlmn/uTWdY++IZba/HFe/TDrmtt53rAJJWl9vIRc6DZsaZtktHKR9vJc1DWU4Fuhxysbultun2uA3vKh5AMvaUr37LSV555tkfPM/acaCTRFoiWWleBEONPXZpPubzUnoCu1pCSCQXyUqlJ9HnzNnzrP2ud72Ttf/2G/9L9XEhP4/VmJ+AoNI2PonrMolElaVSn7ORuF4aHhu/oULW1MWg+twOdsc3jBpiC98waogtfMOoIdMV4kCFrNJ2MdtGmnrC5PE9o4cQNsxSbTudOMGfP587u8Q3yLWgZbPBny2fXOR+gY3VFdUnHfDP1xJxB23PM+1UVM4ppQBFqWMfErFfWckFAHbEA+ntLT625qy2F3uz/AQsLvAEoVIqgADoi+ftI3E+tvu6es3SAvcDJA1dSceJ+1KUcr9Gt6njEF548WnWfvcjb2dt8jznbieLrL0hxttq67GNRvycDca8spGLPfdUEVchE8cAgMTzfxJLNPRUkmLbH1H23u74hlFDbOEbRg2xhW8YNcQWvmHUkKkr8IRMkeZwR4R0bpDTASSyKk7gUelpNbnzKxJKLORRqe1n3Bkm3ZK9pnaOlQOZZMEdUlGgxxYJJ0+RcSdQ7Mm/SRqizLcIiAGAUjhS2z2u9FMGOujn2rU11l5a5tWEwuEJPZht7tzbLrhTkZy+zDIhuFPuaqfvzhZ3mI2Fc6+a0ft95zsfYu3Pfe4LrP0rv/So6vOtb/0Naw92+HG7nopJAxGMM874/AeeJJ2G8Ex3yXNihXe7FMFP1SFJOs5TXciH3fENo4bYwjeMGmIL3zBqyFRt/IACJIG2l/YjAxACEbBQSjlTAHHEAywizyHOLvNt5rrcFq9KrrQKAJUIVqmEmmya6emLGzzJpRRCCiA9uJ0xt9taPW77paW2f8MW90kUQ21Tnlji9vjMArddf/yCFqVogo8/FBWHOh39me/v8sCm9XKTtUvoQJtM+D5c7qlktHw/a69ckQrG+r51XYiCfOiDH2DtKy/pSka9eS7C8tzLvHpxL/ckGYkKuyeXeEDYONPnrBDBQ5WnEhAJdd4sF/N0SCUq3z592B3fMGqILXzDqCG28A2jhkzVxo/CECdmFw/eSApxOClcoW2YdpPbzcunF9Q2p5e5nRkLQYMs00k6VcXtq2aH2/itkRCmBLCbywQPIbbpMcFkDEEgnvWTGAcAxEIstBXpscwIUZAUfC7f+uY3qT5OiFCEQgQkgydeQHyoUyeE4GWf290AUIqKMUWl70Gz89zfsL3Dz9HSko4pIFFtdmubxyqcOKUrDiU9/tqzz4trpaHH1hYxHJUQRklHep7abZ7wFPkStlLuT8hLUWH3FqtN3wy74xtGDbGFbxg15NCFT0RNIvoWEf2AiJ4iot+dvH6RiL5JRM8R0Z8R+eIPDcM4jhzljp8CeK9z7ucBvAPAo0T0CIDfB/Ap59yDADYBfPT1G6ZhGHeSQ517bs9b8GrkRzz55wC8F8C/mrz+OID/CuAPD9oXAfD4SeQReVM4unozuuJKLGJXmg3t4OgJxdk8486YsZK2BZKAO9BK4RyLEj19FInSzsKJJQMyAGCmy7dpCNXgVqQTiEYiYWVpVv/gSpoiKGbMnUvzs7pM8zDhyrs7fe5sijwZQ4V0KA24E9T19UlPRCWdwKMcs7bKHXMyYStNeUASAJy/cJq1Z+dExZ6GHn8U87mc7/H5jj0O5ZMX+FwGxNvJpr5O1zd4kNhgoMefNHm/IOaf+dAknDupwENEIRE9CWAVwFcBvABgy7mfutxXAOgaVIZhHEuOtPCdc6Vz7h0AzgF4F4C3+jbz9SWix4joO0T0nUI8mjAM4+5wS15959wWgK8DeATAHBG9+jvkHICf3KTPZ5xzDzvnHo7CO1MMwDCM18ahNj4RnQCQO+e2iKgF4Jex59j7GoBfB/B5AB8B8KXD9hUGIeY8FVr5AcUAE/5l0ZvlthQAFCJ4IvAIP7z0HFfEjWMRdHJGBxYlwoZfXROBKJ4vsq4YX5VzvwB5Krwmyi7jNrJUdAWAquK/nqKm/g4vRUUhCvg2scdezza53ZkJX4InRwpFJo/N56Xd1Sq1acF9CS3PdfHIL/4z1u52eaDNpZUXVJ+LD/CEoRsbvMJx0tZCLl/84lf4cVp8XopUJ+lkGfcD7OzwBKJulwcfAcCJEzywrNXS22SyCvJIVPk55FdzQEe7lx8lcu80gMdpT+YmAPDnzrmvENHTAD5PRP8NwPcBfPZIRzQM465zFK/+3wF4p+f1F7Fn7xuGcY9hkXuGUUNs4RtGDZl6dt7CnHZo7Ecq8DREFlqq/VwYj4Rqj+f7LHTcYdNr8UCJbFc7TbbWd/gLQj0ojrWjqCHUfEcVz9aLPMFFhZScFRFJ7Z4o9wWgmfDxk6fkciUcaLJkd5Zq52QoLoky50E+g6E+AS1xjoZDPm/Jgr7M5Gwvn9VZcw+8kZc929zk+33j/fx9AOi0+bE6rQus/Tff+Kbqs7khAoVm+TmMAz23SYtnIHaEVzrLdHBUI+H7bXjKhnVn+Gvbr7zE2mmqA8D2Ywo8hmHcFFv4hlFDbOEbRg2hoyp23AkWZnrufY/8owO3iSJuNy+e5CkAa5tavXRDVHJJQm2HxhG3b8cprxgTRTqYZTziY+nMclt7VGiV2jfc9wbWzkbczs5FGwB2+/y1MuB2XO4J+jl9iqvPtCKdvJGLwJMs5UEnQ+lbAJALmz4USjNpoefpJys8sKnT4UFMzpM4Eoky3+VYl9IW+Sp44MFl1j5/mrcB4G//5inW/vHf84DS2RlumwNA0uFroEj5eZ33JIa5iM8dgc9tq6UVkVbXuPpwnunzenKZn9fd3b5o68Se/fzw2ZexOxwfmqljd3zDqCG28A2jhtjCN4waMuVquQ5OqeZypM5AJqqQ5rm28YNI2KWe5+vdWW47FbuiqkmpzaLtdW6Tre3whI+H3n5R9dm4zrfJh9zOTiL+LBcAoga3icuK25yVqtMLjAtuV4elfiZfCMXcsuDzksSe0y9UamX1l9gjCnJaPIOXAhONhqfCMfi8xC3ta8pFgsorz/49az/z1JOqTzvhY3nbQ/w5/tIJbePn4jO2xUe8eoU/SweAfMyv4zVhv8/Nav/J2SWRpCMPBODa6hXWppJf/0tz2t+wnyi8g0IchmH8bGEL3zBqiC18w6ghtvANo4ZM1bkXRzHOLeqgi/0UBXeK3FjnyqRzc1oppxOI0tSlTmR45dKzrN1tcyfJ1cvagba1yx0r//Q9b2btdks7cFpNnmSROD62ly7rAJ6dLV4aKo+5s6nT06W1G8JRR55yTIGQNE4S4dD0nH2ZmBSL0lxhqB1SLeG7i0LubK1yXZ5sZ3Odj82jwNNryXJefCzBWPcpd3mfAfixRwN9nhdPcOfqbs73sbqug2ZOneDX4X0PnhFbeJytBb+e1jZ1abFMJFY1RTKZLGkmkUkWAiFeAAAO50lEQVRuN8Pu+IZRQ2zhG0YNsYVvGDVkugE8gUPR0oql+6lkwIj4auot6mSUhhC3GOzq77PhLg+eCAJuq/5jT/JQKUpCX77ybda+cF4HU6Si3HO7xW3ByFNKyIEnYsgtXKV9CWOR4BFGWtQhJFmVhb/f9yTG5IUUQuG2qgu1vT4ecZ9Kt8n9GqNM250zXT6Y+Xntx0DBP+Nwm9vnUkwFANKC2+NSFVgG6wDA5UtcIXckAocWF7Rf6eoVPnftGe7XSNoeBeOUX09FoddCHIt5IFFJhw6WqHdSpvom2B3fMGqILXzDqCG28A2jhkzVxs/yDJdXvZW2fgoJYcPZGZ7U0u5oGycO+HPYTqJFEE4s8eeshRMiGk7bfsNdbqctznBRkMDp57u9BW5rr13lgh+581RrbXAbvhR2KXkKpAYN7qOoyHMqKz6XZc6fEfsSk4qS297rm9yW9R3GiUo67Zjb3pEnsafV5uesPat9FAAfb1rxc5b1PaIUAZ+sgPixi8LjI4r5NvJSyHOdWNbriWss5CfNVxV5V4iwNBPt16hEFaXhiF8bhye5mdimYRg3wRa+YdQQW/iGUUNs4RtGDZmqc68ogS2Wl6AdEVHEnXcnZ7iiSifWCiopCZXU0lNWWnjMVLVh0pVPKOSBHJ02H9tcT6vp7GzzY6/f4FVaVjd5EAcA3HeaJ/+ksiw26QAeB1HKudLjr1LuTKKQB4yMhtrRuDvg22z2hYJxy+OEE46sWDjYHnyAq+AAQCnKQSPU96DrG3z8G9si4GWsHWiRSCK6scmTvGYW9fUT5Hy/oUiS6s1o52QkxjsWDsAg1uXcA6F4lI21ozHN+Gu5mKdKSlQJzLlnGMZNsYVvGDXEFr5h1JCp2vhh4NBtH1ztMwy4TZNnXKxgZ0sPOerwBJzhaEdtE4jkhbLkNnGrqQODsowHiLiA2/zXruoAEhlTMsx4QIbzROP0ha3a63CBicSjUtvocPu8rLTvwEHY/SL6ZnegKwFlY5GE47hNn6c+G5PblS++dJW1xyM9T0tLPAAmzbVtekP4Q9KUXxuRJ2ElEVldJJJeysoTANYSgUzCATQutP+kIwOxxPDbHW3jt4n7hK5du662GY/5NdZq8fFH4cFLNgiOdi+3O75h1BBb+IZRQ4688IkoJKLvE9FXJu2LRPRNInqOiP6MiPSzIcMwjiW3YuN/DMAzAF5VWfh9AJ9yzn2eiP4HgI8C+MODdkBBjEbrYLFNSeF4wkfcPKW2qTL+MSjzVJ7JuN0vK/LsbHl8D8LfsLTIq/G8ct0ThyAShgYD/nz3woUHVJ8TJ3msQime5Tr5zBvAQIhFFIX+zLEYfxhxW3b1+mXVpymqvEpfCDyiIKWsoOu4Lbu6puc2K+X50KIUY+G2IHB/Q9DQMQXOCRFSYYsHlba9t1ZFrEKTX0+Nrvax7IjBhaL6784Ojx8AtP0deB65O1EZOU25z6UIDxHi8CSb+TjSHZ+IzgH4VQB/NGkTgPcC+IvJJo8D+NCRjmgYxl3nqD/1Pw3gdwC8+nWyCGDL/UOO4AqAs76ORPQYEX2HiL6TZto7ahjG9Dl04RPRBwCsOue+u/9lz6beWEHn3Geccw875x72PZYyDGP6HMXGfzeAXyOi9wNoYs/G/zSAOSKKJnf9cwAOVtgwDOPYcOjCd859EsAnAYCI3gPgPzrnfpOIvgDg1wF8HsBHAHzpsH21Wk287R1vuaUBBiJBJfH8RklH3NEy6F9T22yv88ot45R/9LClH0qsrq+w9nCXO9DKQqu8xi3uyHrobQ+y9voGV+QBgI0bojS1MImWT3KnIgBUIhkl8wTWBCLxKIz4j7KFOa6GC2iHWiDKWXc7vsQkHqATNUQlHad/IG6IIjJZoYN8nOODWZjn89Bqe1R2R7xcdbMtlYp00NLMDD/3UuVplPJEKwCYO8Udi2UhP7NOFCtzPg8nFrV67wMX+foYCgXjPPeYy/t2+9RzV/T7Hl7Lc/yPA/gPRPQ89mz+z76GfRmGMUVuKWTXOfd1AF+f/P9FAO+680MyDOP1xiL3DKOGkHNHS9y/E4RErnnIb4yFeR5A8q8//AHWfubpH6o+Wzs8GOTiaS22MNvm9tSVK9x+XB/qxJ5/8Sv/nLW/8X+eYO12rKu1zixwm7I/5HYdQdvIjrjgxM+97T7W7rT09/OlFW53vvLyJbUNhdzOnJ3lNvHuhrZ3B30+3s4yt6t/9OTTqo8UDqGW8C141GSbsZiHSn/GKOK+j9GQj3c81uNPhMJvGHB7fXZWKzCfOnWatdfWhB+G9BrJSn7O5kVF4zMnl1SfRix8ByNdOTkScxUn3JcQHhLA84UvfQ2ra5uHltOxO75h1BBb+IZRQ2zhG0YNmaoQR6fbwiO/8MYDt5E2zKXLPJGk6xE4WBbPVINQP+vcHvBn8LmoWhuU2g797jf+lrWTituH2/2XVZ9Wl29z9uT9rP3SK/o5fq/Lxz/Y4c+0tzf0M/r5OW57vxjohJtr13k8Q1acZO1OzJODACDsCbuTuLmYesQvmh0+d1UkxuvJRsmEOEfS0IKWEAmfCye576bnSdjq97m/4cYqT5bZWNe+nO0tnggTiGSmdkePbSbiMRD9He4zemZHx7MlDb7fVlvfd2cXhaDrLD9OEB8S/RpYtVzDMG6CLXzDqCG28A2jhtjCN4waMlXnXhwHOHVSO+cYohJIIYI0yFMpxAmVnnSsP1ZRcAdNLqq//OK7f0H1efnp77H2eIf3uXiGB9oAQHuWB/W89NJLrN2b44rAALAtqr28+MKLrL18mgeYAMDb38HLfleVdvrsDrmTrdUWijyhdgS1m9yBdvkSd1J12/o4rYQ7pIZCDRek7y9VwveTO63sE4nxZTn/PLtSRRhAXknVIX7syDOWGaFkSyIxjFTZJaAhEp7SnM9BlurjdBe5M7W/rVV6Zmf4ZxYV1OGkIpLEKukYhnEzbOEbRg2xhW8YNWTKlXQizHZOHrhNLkQoLq9wYYGGR74raPLvr9iTPANwG78ouMDE+oauahK2uN0WprxP5KlaEgbc39Cb4QEYM7OexJ72edZOB/wzdzs6sWR7myfphB518/l5rmh84iT3C8x1ddBSMRSJPV1ulzaauk/c4Lb3Aws8IWrlihaHyIUt2pcliKBt4DDgn3HsCfqRlXQaCb/EW4kW7whLPv5ZESTWjnXQ0jjjxnfc4UFY/UgrC88v8mMHfb38hv1t1l4XYzssPKfMtQCID7vjG0YNsYVvGDXEFr5h1BBb+IZRQ6bq3CurEjtjrVi6n0I4JwqhItNr6wCY0Yg7QAYDrWziSp6dd+Y0d3yRZyoaXZ4B58AVXEeFLvtU7PJtXMXHtr4m5GUBnFjkzqQ3vplnnRWOO3wA4MYmL0Xd7mjn3gNzPMCo2+YOqGasXUWzS/wzJ03ukFrzqPbMLnDn48baK6zd8ojGBCKBr9HVgV3ZiN+XKlGqi0Lt3AuFozGvuLM4TfW1kYgAnpEoFU65/gDUEuXDc77fzOnj7PS5Q7AsPOrDfZ49eGODr5foEAWezKfC68Hu+IZRQ2zhG0YNsYVvGDVkqjY+qEIZaNuHIUzVpWVR7SXXdtGgz4NBslyXjJbCJM2WsA89uQ1hgwfbhAnfL0EHSxQlt/tXVrgyTtLW1WvmZrnibCKUVcuS25x7++WVgTozOjDq3DkeSLOzyf0LrQWt+Du/wMc3zvjExA2t2iNVYa5deYFv4CmtLSp4o93WgUG7KbdXM1GRx7NbZQPLM5RnOrAmFeNPx/w8R03tS2iK4KfBLj9HWzvalxUl/OKWwV4AIE30WJSOkmpHElmK+6bbHWkrwzB+prCFbxg1xBa+YdQQW/iGUUNs4RtGDbGFbxg1xBa+YdQQW/iGUUNs4RtGDbGFbxg1xBa+YdQQW/iGUUPIuaNV3rgjByNaA/AKgCUA64dsfly4l8YK3FvjvZfGCtwb473POXfisI2muvB/elCi7zjnHp76gW+De2mswL013ntprMC9N96DsJ/6hlFDbOEbRg25Wwv/M3fpuLfDvTRW4N4a7700VuDeG+9NuSs2vmEYdxf7qW8YNWSqC5+IHiWiHxPR80T0iWke+ygQ0R8T0SoR/WjfawtE9FUiem7yd/5ujvFViOg8EX2NiJ4hoqeI6GOT14/reJtE9C0i+sFkvL87ef0iEX1zMt4/I/JU/7xLEFFIRN8noq9M2sd2rLfK1BY+EYUA/juAXwHwEIAPE9FD0zr+EfkTAI+K1z4B4Ann3IMAnpi0jwMFgN92zr0VwCMA/t1kPo/reFMA73XO/TyAdwB4lIgeAfD7AD41Ge8mgI/exTFKPgbgmX3t4zzWW2Kad/x3AXjeOfeicy4D8HkAH5zi8Q/FOffXAGSpmw8CeHzy/8cBfGiqg7oJzrmrzrnvTf7fx94FehbHd7zOOfdqLex48s8BeC+Av5i8fmzGS0TnAPwqgD+atAnHdKy3wzQX/lkA+7WmVyavHXdOOeeuAnuLDYDWsb7LENH9AN4J4Js4xuOd/HR+EsAqgK8CeAHAlnPuVRXs43RNfBrA7wB4tSbXIo7vWG+ZaS58LYjvVbM3bgUi6gL4IoDfcs7tHLb93cQ5Vzrn3gHgHPZ+Ab7Vt9l0R6Uhog8AWHXOfXf/y55N7/pYb5dpFtRYAXB+X/scgJ9M8fi3y3UiOu2cu0pEp7F3tzoWEFGMvUX/p865v5y8fGzH+yrOuS0i+jr2fBNzRBRN7qTH5Zp4N4BfI6L3A2gCmMHeL4DjONbbYpp3/G8DeHDiGW0A+A0AX57i8W+XLwP4yOT/HwHwpbs4lp8ysTk/C+AZ59wf7HvruI73BBHNTf7fAvDL2PNLfA3Ar082Oxbjdc590jl3zjl3P/au0//tnPtNHMOx3jbOuan9A/B+AM9iz7b7z9M89hHH9zkAVwHk2PuF8lHs2XZPAHhu8nfhbo9zMtZ/gr2fmn8H4MnJv/cf4/G+HcD3J+P9EYD/Mnn9AQDfAvA8gC8ASO72WMW43wPgK/fCWG/ln0XuGUYNscg9w6ghtvANo4bYwjeMGmIL3zBqiC18w6ghtvANo4bYwjeMGmIL3zBqyP8H13jOBnq7YBgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = training_set.next()\n",
    "for i in range(0,1):\n",
    "    random_image = x[i]\n",
    "    plt.imshow(random_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an earlystopping callback\n",
    "* Import EarlyStopping - method to stop training when a monitored quantity has stopped improving.\n",
    "* Define a callback.Set monitor as val_acc, patience as 5 and mode as max so that if val_acc does not improve over 5 epochs, terminate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# acc_callback = [EarlyStopping(monitor='val_acc', patience=5, mode='max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "* Invoke the fit_generator to fits the model on data generated batch-by-batch by a Python generator.\n",
    "    * steps_per_epoch’ holds the number of training images, i.e 8000\n",
    "    * A single epoch is a single step in training a neural network,set it at 25.\n",
    "    * callbacks: List of callbacks to apply during training.\n",
    "    * validation_data: test data\n",
    "    * validation_steps: Total number of steps (batches of samples) to yield from  validation_data generator before stopping at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 2132s 267ms/step - loss: 0.3365 - acc: 0.8428 - val_loss: 0.6525 - val_acc: 0.7924\n",
      "Epoch 2/25\n",
      "2772/8000 [=========>....................] - ETA: 10:53 - loss: 0.1012 - acc: 0.9626"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-699473eb91ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#callbacks = acc_callback,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m validation_steps = 2000)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "steps_per_epoch = 8000,\n",
    "epochs = 25,\n",
    "#callbacks = acc_callback,\n",
    "validation_data = test_set,\n",
    "validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "* Load model from disk.\n",
    "* Preprocess and feed a random input image to the model for prediction.\n",
    "* Test the accuracy and loss using the evaluate_generator method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('saved_models/cnn_base_model.json', 'r')\n",
    "\n",
    "loaded_classifier_json = json_file.read()\n",
    "\n",
    "json_file.close()\n",
    "\n",
    "loaded_classifier = model_from_json(loaded_classifier_json)\n",
    "\n",
    "loaded_classifier.load_weights(\"saved_models/cnn_base_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction = 'This is a dog'\n",
    "else:\n",
    "    prediction = 'This is a cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = loaded_classifier.evaluate_generator(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization tools\n",
    "* plot_model : The keras.utils.vis_utils module provides utility functions to plot a Keras model (using graphviz).This will plot a graph of the model and save it to a file.\n",
    "* quiver_engine : Interactive deep convolutional networks features visualization.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(classifier, to_file='classifier.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiver_engine import server\n",
    "server.launch(loaded_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
